diff --git a/V2/output.png b/V2/output.png
index ab7d89c..b7baa79 100644
Binary files a/V2/output.png and b/V2/output.png differ
diff --git a/V2/param-tunning.ipynb b/V2/param-tunning.ipynb
index 62c7a8a..4e0d687 100644
--- a/V2/param-tunning.ipynb
+++ b/V2/param-tunning.ipynb
@@ -134,7 +134,7 @@
     "        record[\"annotations\"] = objs\n",
     "        new_data.append(record)\n",
     "    np.random.shuffle(new_data)\n",
-    "#     np.save(\"data.npy\", new_data)\n",
+    "    np.save(\"data.npy\", new_data)\n",
     "    return new_data"
    ]
   },
@@ -214,7 +214,7 @@
     "MAX_ITER = 500\n",
     "EVAL_PERIOD = 500\n",
     "IMS_PER_BATCH = 2\n",
-    "BATCH_SIZE_PER_IMAGE = 16\n",
+    "BATCH_SIZE_PER_IMAGE = 128\n",
     "SCORE_THRESH_TEST = 0.625\n",
     "model = f\"COCO-Detection/\" + \"faster_rcnn_X_101_32x8d_FPN_3x.yaml\""
    ]
@@ -231,7 +231,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 11,
    "id": "6a2539e4-017c-4bbd-99bd-439f1712a127",
    "metadata": {},
    "outputs": [
@@ -812,7 +812,213 @@
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      " 18%|███████▎                                | 341/1853 [00:21<00:54, 27.76it/s]"
+      "100%|███████████████████████████████████████| 1853/1853 [01:51<00:00, 16.57it/s]"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\u001b[32m[10/28 20:13:09 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 1853 images left.\n",
+      "\u001b[32m[10/28 20:13:09 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
+      "\u001b[36m|  category  | #instances   |\n",
+      "|:----------:|:-------------|\n",
+      "|    Card    | 1853         |\n",
+      "|            |              |\u001b[0m\n",
+      "\u001b[32m[10/28 20:13:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
+      "\u001b[32m[10/28 20:13:09 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
+      "\u001b[32m[10/28 20:13:09 d2.data.common]: \u001b[0mSerializing 1853 elements to byte tensors and concatenating them all ...\n",
+      "\u001b[32m[10/28 20:13:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.35 MiB\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\n",
+      "2021-10-28 20:13:09.678164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
+      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
+      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
+      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
+      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
+      "Some model parameters or buffers are not found in the checkpoint:\n",
+      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
+      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
+      "The checkpoint state_dict contains keys that are not used by the model:\n",
+      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\u001b[32m[10/28 20:13:13 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
+      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
+      "  return torch.floor_divide(self, other)\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\u001b[32m[10/28 20:13:36 d2.utils.events]: \u001b[0m eta: 0:08:28  iter: 19  total_loss: 1.535  loss_cls: 0.6501  loss_box_reg: 0.8407  loss_rpn_cls: 0.03065  loss_rpn_loc: 0.005025  time: 1.1147  data_time: 0.0259  lr: 9.7405e-06  max_mem: 5849M\n",
+      "\u001b[32m[10/28 20:13:59 d2.utils.events]: \u001b[0m eta: 0:08:01  iter: 39  total_loss: 1.463  loss_cls: 0.6092  loss_box_reg: 0.805  loss_rpn_cls: 0.01963  loss_rpn_loc: 0.008261  time: 1.1189  data_time: 0.0026  lr: 1.9731e-05  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:14:23 d2.utils.events]: \u001b[0m eta: 0:07:48  iter: 59  total_loss: 1.436  loss_cls: 0.5518  loss_box_reg: 0.8227  loss_rpn_cls: 0.02975  loss_rpn_loc: 0.006296  time: 1.1418  data_time: 0.0025  lr: 2.972e-05  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:14:43 d2.utils.events]: \u001b[0m eta: 0:07:13  iter: 79  total_loss: 1.426  loss_cls: 0.5197  loss_box_reg: 0.8356  loss_rpn_cls: 0.01727  loss_rpn_loc: 0.006172  time: 1.1054  data_time: 0.0027  lr: 3.9711e-05  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:15:02 d2.utils.events]: \u001b[0m eta: 0:06:41  iter: 99  total_loss: 1.374  loss_cls: 0.4657  loss_box_reg: 0.83  loss_rpn_cls: 0.03329  loss_rpn_loc: 0.007208  time: 1.0783  data_time: 0.0028  lr: 4.9701e-05  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:15:24 d2.utils.events]: \u001b[0m eta: 0:06:26  iter: 119  total_loss: 1.321  loss_cls: 0.4515  loss_box_reg: 0.7493  loss_rpn_cls: 0.03836  loss_rpn_loc: 0.006119  time: 1.0781  data_time: 0.0024  lr: 5.9691e-05  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:15:48 d2.utils.events]: \u001b[0m eta: 0:06:09  iter: 139  total_loss: 1.286  loss_cls: 0.4533  loss_box_reg: 0.7835  loss_rpn_cls: 0.04044  loss_rpn_loc: 0.007496  time: 1.0952  data_time: 0.0027  lr: 6.9681e-05  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:16:09 d2.utils.events]: \u001b[0m eta: 0:05:48  iter: 159  total_loss: 1.152  loss_cls: 0.4271  loss_box_reg: 0.6855  loss_rpn_cls: 0.01476  loss_rpn_loc: 0.006112  time: 1.0946  data_time: 0.0023  lr: 7.9671e-05  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:16:30 d2.utils.events]: \u001b[0m eta: 0:05:26  iter: 179  total_loss: 1.232  loss_cls: 0.4401  loss_box_reg: 0.7665  loss_rpn_cls: 0.02903  loss_rpn_loc: 0.006973  time: 1.0887  data_time: 0.0025  lr: 8.966e-05  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:16:53 d2.utils.events]: \u001b[0m eta: 0:05:06  iter: 199  total_loss: 1.267  loss_cls: 0.3896  loss_box_reg: 0.8387  loss_rpn_cls: 0.01676  loss_rpn_loc: 0.008994  time: 1.0919  data_time: 0.0025  lr: 9.9651e-05  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:17:15 d2.utils.events]: \u001b[0m eta: 0:04:46  iter: 219  total_loss: 1.286  loss_cls: 0.4225  loss_box_reg: 0.7659  loss_rpn_cls: 0.02721  loss_rpn_loc: 0.004732  time: 1.0944  data_time: 0.0026  lr: 0.00010964  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:17:37 d2.utils.events]: \u001b[0m eta: 0:04:26  iter: 239  total_loss: 1.17  loss_cls: 0.4279  loss_box_reg: 0.5677  loss_rpn_cls: 0.04804  loss_rpn_loc: 0.00845  time: 1.0960  data_time: 0.0025  lr: 0.00011963  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:18:00 d2.utils.events]: \u001b[0m eta: 0:04:05  iter: 259  total_loss: 1.09  loss_cls: 0.3451  loss_box_reg: 0.7291  loss_rpn_cls: 0.01593  loss_rpn_loc: 0.005633  time: 1.0991  data_time: 0.0025  lr: 0.00012962  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:18:22 d2.utils.events]: \u001b[0m eta: 0:03:45  iter: 279  total_loss: 1.226  loss_cls: 0.3738  loss_box_reg: 0.7748  loss_rpn_cls: 0.01202  loss_rpn_loc: 0.007309  time: 1.0988  data_time: 0.0024  lr: 0.00013961  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:18:45 d2.utils.events]: \u001b[0m eta: 0:03:25  iter: 299  total_loss: 1.166  loss_cls: 0.3772  loss_box_reg: 0.7213  loss_rpn_cls: 0.02017  loss_rpn_loc: 0.008634  time: 1.1035  data_time: 0.0023  lr: 0.0001496  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:19:07 d2.utils.events]: \u001b[0m eta: 0:03:05  iter: 319  total_loss: 1.142  loss_cls: 0.397  loss_box_reg: 0.7092  loss_rpn_cls: 0.02757  loss_rpn_loc: 0.007529  time: 1.1032  data_time: 0.0024  lr: 0.00015959  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:19:30 d2.utils.events]: \u001b[0m eta: 0:02:44  iter: 339  total_loss: 1.154  loss_cls: 0.345  loss_box_reg: 0.7614  loss_rpn_cls: 0.009385  loss_rpn_loc: 0.009088  time: 1.1041  data_time: 0.0024  lr: 0.00016958  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:19:51 d2.utils.events]: \u001b[0m eta: 0:02:24  iter: 359  total_loss: 0.9966  loss_cls: 0.3399  loss_box_reg: 0.5781  loss_rpn_cls: 0.02267  loss_rpn_loc: 0.007779  time: 1.1014  data_time: 0.0025  lr: 0.00017957  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:20:15 d2.utils.events]: \u001b[0m eta: 0:02:03  iter: 379  total_loss: 0.9412  loss_cls: 0.3625  loss_box_reg: 0.574  loss_rpn_cls: 0.01593  loss_rpn_loc: 0.008482  time: 1.1059  data_time: 0.0027  lr: 0.00018956  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:20:35 d2.utils.events]: \u001b[0m eta: 0:01:42  iter: 399  total_loss: 0.9219  loss_cls: 0.3092  loss_box_reg: 0.6028  loss_rpn_cls: 0.02741  loss_rpn_loc: 0.009134  time: 1.1016  data_time: 0.0026  lr: 0.00019955  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:20:58 d2.utils.events]: \u001b[0m eta: 0:01:22  iter: 419  total_loss: 0.8224  loss_cls: 0.3272  loss_box_reg: 0.4508  loss_rpn_cls: 0.01638  loss_rpn_loc: 0.009789  time: 1.1047  data_time: 0.0024  lr: 0.00020954  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:21:21 d2.utils.events]: \u001b[0m eta: 0:01:02  iter: 439  total_loss: 0.814  loss_cls: 0.2779  loss_box_reg: 0.5318  loss_rpn_cls: 0.01625  loss_rpn_loc: 0.005275  time: 1.1056  data_time: 0.0027  lr: 0.00021953  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:21:43 d2.utils.events]: \u001b[0m eta: 0:00:41  iter: 459  total_loss: 0.8332  loss_cls: 0.257  loss_box_reg: 0.5001  loss_rpn_cls: 0.01703  loss_rpn_loc: 0.004427  time: 1.1059  data_time: 0.0026  lr: 0.00022952  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:22:07 d2.utils.events]: \u001b[0m eta: 0:00:20  iter: 479  total_loss: 0.8354  loss_cls: 0.3279  loss_box_reg: 0.4915  loss_rpn_cls: 0.02325  loss_rpn_loc: 0.005287  time: 1.1100  data_time: 0.0023  lr: 0.00023951  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:22:30 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 499  total_loss: 0.8985  loss_cls: 0.3209  loss_box_reg: 0.5977  loss_rpn_cls: 0.02255  loss_rpn_loc: 0.006047  time: 1.1102  data_time: 0.0025  lr: 0.0002495  max_mem: 6042M\n",
+      "\u001b[32m[10/28 20:22:30 d2.engine.hooks]: \u001b[0mOverall training speed: 498 iterations in 0:09:12 (1.1102 s / it)\n",
+      "\u001b[32m[10/28 20:22:30 d2.engine.hooks]: \u001b[0mTotal training time: 0:09:13 (0:00:00 on hooks)\n",
+      "\u001b[32m[10/28 20:22:32 d2.evaluation.coco_evaluation]: \u001b[0m'test' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...\n",
+      "\u001b[32m[10/28 20:22:32 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'test' to COCO format ...)\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "100%|███████████████████████████████████████| 1853/1853 [01:53<00:00, 16.30it/s]"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\u001b[32m[10/28 20:24:26 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\u001b[32m[10/28 20:24:26 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 1853, #annotations: 1853\n",
+      "\u001b[32m[10/28 20:24:26 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at './output/test_coco_format.json' ...\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "100%|███████████████████████████████████████| 1853/1853 [01:52<00:00, 16.42it/s]"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\u001b[32m[10/28 20:26:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
+      "\u001b[32m[10/28 20:26:19 d2.data.common]: \u001b[0mSerializing 1853 elements to byte tensors and concatenating them all ...\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\u001b[32m[10/28 20:26:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.35 MiB\n",
+      "\u001b[32m[10/28 20:26:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 1853 batches\n",
+      "\u001b[32m[10/28 20:26:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/1853. Dataloading: 0.0006 s/iter. Inference: 0.1486 s/iter. Eval: 0.0001 s/iter. Total: 0.1494 s/iter. ETA=0:04:35\n",
+      "\u001b[32m[10/28 20:26:27 d2.evaluation.evaluator]: \u001b[0mInference done 39/1853. Dataloading: 0.0013 s/iter. Inference: 0.1747 s/iter. Eval: 0.0001 s/iter. Total: 0.1763 s/iter. ETA=0:05:19\n",
+      "\u001b[32m[10/28 20:26:32 d2.evaluation.evaluator]: \u001b[0mInference done 66/1853. Dataloading: 0.0024 s/iter. Inference: 0.1808 s/iter. Eval: 0.0001 s/iter. Total: 0.1834 s/iter. ETA=0:05:27\n",
+      "\u001b[32m[10/28 20:26:37 d2.evaluation.evaluator]: \u001b[0mInference done 94/1853. Dataloading: 0.0048 s/iter. Inference: 0.1776 s/iter. Eval: 0.0001 s/iter. Total: 0.1826 s/iter. ETA=0:05:21\n",
+      "\u001b[32m[10/28 20:26:42 d2.evaluation.evaluator]: \u001b[0mInference done 122/1853. Dataloading: 0.0038 s/iter. Inference: 0.1785 s/iter. Eval: 0.0001 s/iter. Total: 0.1826 s/iter. ETA=0:05:16\n",
+      "\u001b[32m[10/28 20:26:47 d2.evaluation.evaluator]: \u001b[0mInference done 149/1853. Dataloading: 0.0043 s/iter. Inference: 0.1793 s/iter. Eval: 0.0001 s/iter. Total: 0.1839 s/iter. ETA=0:05:13\n",
+      "\u001b[32m[10/28 20:26:52 d2.evaluation.evaluator]: \u001b[0mInference done 177/1853. Dataloading: 0.0038 s/iter. Inference: 0.1797 s/iter. Eval: 0.0001 s/iter. Total: 0.1837 s/iter. ETA=0:05:07\n",
+      "\u001b[32m[10/28 20:26:57 d2.evaluation.evaluator]: \u001b[0mInference done 203/1853. Dataloading: 0.0057 s/iter. Inference: 0.1792 s/iter. Eval: 0.0001 s/iter. Total: 0.1851 s/iter. ETA=0:05:05\n",
+      "\u001b[32m[10/28 20:27:03 d2.evaluation.evaluator]: \u001b[0mInference done 231/1853. Dataloading: 0.0051 s/iter. Inference: 0.1791 s/iter. Eval: 0.0001 s/iter. Total: 0.1844 s/iter. ETA=0:04:59\n",
+      "\u001b[32m[10/28 20:27:08 d2.evaluation.evaluator]: \u001b[0mInference done 260/1853. Dataloading: 0.0046 s/iter. Inference: 0.1787 s/iter. Eval: 0.0001 s/iter. Total: 0.1836 s/iter. ETA=0:04:52\n",
+      "\u001b[32m[10/28 20:27:13 d2.evaluation.evaluator]: \u001b[0mInference done 287/1853. Dataloading: 0.0043 s/iter. Inference: 0.1796 s/iter. Eval: 0.0001 s/iter. Total: 0.1840 s/iter. ETA=0:04:48\n",
+      "\u001b[32m[10/28 20:27:18 d2.evaluation.evaluator]: \u001b[0mInference done 315/1853. Dataloading: 0.0040 s/iter. Inference: 0.1799 s/iter. Eval: 0.0001 s/iter. Total: 0.1841 s/iter. ETA=0:04:43\n",
+      "\u001b[32m[10/28 20:27:23 d2.evaluation.evaluator]: \u001b[0mInference done 342/1853. Dataloading: 0.0037 s/iter. Inference: 0.1803 s/iter. Eval: 0.0001 s/iter. Total: 0.1842 s/iter. ETA=0:04:38\n",
+      "\u001b[32m[10/28 20:27:28 d2.evaluation.evaluator]: \u001b[0mInference done 369/1853. Dataloading: 0.0035 s/iter. Inference: 0.1809 s/iter. Eval: 0.0001 s/iter. Total: 0.1846 s/iter. ETA=0:04:33\n",
+      "\u001b[32m[10/28 20:27:33 d2.evaluation.evaluator]: \u001b[0mInference done 396/1853. Dataloading: 0.0033 s/iter. Inference: 0.1814 s/iter. Eval: 0.0001 s/iter. Total: 0.1850 s/iter. ETA=0:04:29\n",
+      "\u001b[32m[10/28 20:27:38 d2.evaluation.evaluator]: \u001b[0mInference done 424/1853. Dataloading: 0.0033 s/iter. Inference: 0.1812 s/iter. Eval: 0.0001 s/iter. Total: 0.1847 s/iter. ETA=0:04:23\n",
+      "\u001b[32m[10/28 20:27:43 d2.evaluation.evaluator]: \u001b[0mInference done 451/1853. Dataloading: 0.0036 s/iter. Inference: 0.1811 s/iter. Eval: 0.0001 s/iter. Total: 0.1849 s/iter. ETA=0:04:19\n",
+      "\u001b[32m[10/28 20:27:48 d2.evaluation.evaluator]: \u001b[0mInference done 479/1853. Dataloading: 0.0035 s/iter. Inference: 0.1810 s/iter. Eval: 0.0001 s/iter. Total: 0.1847 s/iter. ETA=0:04:13\n",
+      "\u001b[32m[10/28 20:27:54 d2.evaluation.evaluator]: \u001b[0mInference done 507/1853. Dataloading: 0.0033 s/iter. Inference: 0.1811 s/iter. Eval: 0.0001 s/iter. Total: 0.1846 s/iter. ETA=0:04:08\n",
+      "\u001b[32m[10/28 20:27:59 d2.evaluation.evaluator]: \u001b[0mInference done 535/1853. Dataloading: 0.0032 s/iter. Inference: 0.1810 s/iter. Eval: 0.0001 s/iter. Total: 0.1844 s/iter. ETA=0:04:03\n",
+      "\u001b[32m[10/28 20:28:04 d2.evaluation.evaluator]: \u001b[0mInference done 563/1853. Dataloading: 0.0031 s/iter. Inference: 0.1809 s/iter. Eval: 0.0001 s/iter. Total: 0.1841 s/iter. ETA=0:03:57\n",
+      "\u001b[32m[10/28 20:28:09 d2.evaluation.evaluator]: \u001b[0mInference done 592/1853. Dataloading: 0.0030 s/iter. Inference: 0.1807 s/iter. Eval: 0.0001 s/iter. Total: 0.1839 s/iter. ETA=0:03:51\n",
+      "\u001b[32m[10/28 20:28:14 d2.evaluation.evaluator]: \u001b[0mInference done 620/1853. Dataloading: 0.0030 s/iter. Inference: 0.1808 s/iter. Eval: 0.0001 s/iter. Total: 0.1840 s/iter. ETA=0:03:46\n",
+      "\u001b[32m[10/28 20:28:19 d2.evaluation.evaluator]: \u001b[0mInference done 647/1853. Dataloading: 0.0032 s/iter. Inference: 0.1808 s/iter. Eval: 0.0001 s/iter. Total: 0.1841 s/iter. ETA=0:03:42\n",
+      "\u001b[32m[10/28 20:28:24 d2.evaluation.evaluator]: \u001b[0mInference done 674/1853. Dataloading: 0.0031 s/iter. Inference: 0.1809 s/iter. Eval: 0.0001 s/iter. Total: 0.1842 s/iter. ETA=0:03:37\n",
+      "\u001b[32m[10/28 20:28:29 d2.evaluation.evaluator]: \u001b[0mInference done 702/1853. Dataloading: 0.0030 s/iter. Inference: 0.1809 s/iter. Eval: 0.0001 s/iter. Total: 0.1841 s/iter. ETA=0:03:31\n",
+      "\u001b[32m[10/28 20:28:34 d2.evaluation.evaluator]: \u001b[0mInference done 728/1853. Dataloading: 0.0034 s/iter. Inference: 0.1808 s/iter. Eval: 0.0001 s/iter. Total: 0.1844 s/iter. ETA=0:03:27\n",
+      "\u001b[32m[10/28 20:28:39 d2.evaluation.evaluator]: \u001b[0mInference done 755/1853. Dataloading: 0.0033 s/iter. Inference: 0.1810 s/iter. Eval: 0.0001 s/iter. Total: 0.1845 s/iter. ETA=0:03:22\n",
+      "\u001b[32m[10/28 20:28:44 d2.evaluation.evaluator]: \u001b[0mInference done 783/1853. Dataloading: 0.0034 s/iter. Inference: 0.1810 s/iter. Eval: 0.0001 s/iter. Total: 0.1846 s/iter. ETA=0:03:17\n",
+      "\u001b[32m[10/28 20:28:50 d2.evaluation.evaluator]: \u001b[0mInference done 811/1853. Dataloading: 0.0036 s/iter. Inference: 0.1808 s/iter. Eval: 0.0001 s/iter. Total: 0.1845 s/iter. ETA=0:03:12\n",
+      "\u001b[32m[10/28 20:28:55 d2.evaluation.evaluator]: \u001b[0mInference done 839/1853. Dataloading: 0.0035 s/iter. Inference: 0.1808 s/iter. Eval: 0.0001 s/iter. Total: 0.1844 s/iter. ETA=0:03:07\n",
+      "\u001b[32m[10/28 20:29:00 d2.evaluation.evaluator]: \u001b[0mInference done 866/1853. Dataloading: 0.0034 s/iter. Inference: 0.1809 s/iter. Eval: 0.0001 s/iter. Total: 0.1845 s/iter. ETA=0:03:02\n",
+      "\u001b[32m[10/28 20:29:05 d2.evaluation.evaluator]: \u001b[0mInference done 894/1853. Dataloading: 0.0034 s/iter. Inference: 0.1809 s/iter. Eval: 0.0001 s/iter. Total: 0.1845 s/iter. ETA=0:02:56\n",
+      "\u001b[32m[10/28 20:29:10 d2.evaluation.evaluator]: \u001b[0mInference done 922/1853. Dataloading: 0.0033 s/iter. Inference: 0.1810 s/iter. Eval: 0.0001 s/iter. Total: 0.1845 s/iter. ETA=0:02:51\n",
+      "\u001b[32m[10/28 20:29:15 d2.evaluation.evaluator]: \u001b[0mInference done 950/1853. Dataloading: 0.0032 s/iter. Inference: 0.1810 s/iter. Eval: 0.0001 s/iter. Total: 0.1844 s/iter. ETA=0:02:46\n",
+      "\u001b[32m[10/28 20:29:20 d2.evaluation.evaluator]: \u001b[0mInference done 977/1853. Dataloading: 0.0032 s/iter. Inference: 0.1812 s/iter. Eval: 0.0001 s/iter. Total: 0.1845 s/iter. ETA=0:02:41\n",
+      "\u001b[32m[10/28 20:29:25 d2.evaluation.evaluator]: \u001b[0mInference done 1004/1853. Dataloading: 0.0032 s/iter. Inference: 0.1813 s/iter. Eval: 0.0001 s/iter. Total: 0.1847 s/iter. ETA=0:02:36\n",
+      "\u001b[32m[10/28 20:29:31 d2.evaluation.evaluator]: \u001b[0mInference done 1031/1853. Dataloading: 0.0032 s/iter. Inference: 0.1814 s/iter. Eval: 0.0001 s/iter. Total: 0.1849 s/iter. ETA=0:02:31\n",
+      "\u001b[32m[10/28 20:29:36 d2.evaluation.evaluator]: \u001b[0mInference done 1058/1853. Dataloading: 0.0032 s/iter. Inference: 0.1815 s/iter. Eval: 0.0001 s/iter. Total: 0.1849 s/iter. ETA=0:02:27\n",
+      "\u001b[32m[10/28 20:29:41 d2.evaluation.evaluator]: \u001b[0mInference done 1086/1853. Dataloading: 0.0032 s/iter. Inference: 0.1815 s/iter. Eval: 0.0001 s/iter. Total: 0.1848 s/iter. ETA=0:02:21\n",
+      "\u001b[32m[10/28 20:29:46 d2.evaluation.evaluator]: \u001b[0mInference done 1112/1853. Dataloading: 0.0032 s/iter. Inference: 0.1816 s/iter. Eval: 0.0001 s/iter. Total: 0.1851 s/iter. ETA=0:02:17\n",
+      "\u001b[32m[10/28 20:29:51 d2.evaluation.evaluator]: \u001b[0mInference done 1127/1853. Dataloading: 0.0032 s/iter. Inference: 0.1841 s/iter. Eval: 0.0001 s/iter. Total: 0.1875 s/iter. ETA=0:02:16\n"
+     ]
+    },
+    {
+     "ename": "RuntimeError",
+     "evalue": "DataLoader worker (pid 556317) is killed by signal: Killed. ",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
+      "\u001b[0;32m/tmp/ipykernel_544597/115025483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCOEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./output/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_detection_test_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/evaluation/evaluator.py\u001b[0m in \u001b[0;36minference_on_dataset\u001b[0;34m(model, data_loader, evaluator)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mstart_compute_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetected_instances\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_generator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;34m\"proposals\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         proposals = self.predict_proposals(\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_objectness_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_anchor_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         )\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py\u001b[0m in \u001b[0;36mpredict_proposals\u001b[0;34m(self, anchors, pred_objectness_logits, pred_anchor_deltas, image_sizes)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mpred_proposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_anchor_deltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             return find_top_rpn_proposals(\n\u001b[0m\u001b[1;32m    504\u001b[0m                 \u001b[0mpred_proposals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0mpred_objectness_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/proposal_utils.py\u001b[0m in \u001b[0;36mfind_top_rpn_proposals\u001b[0;34m(proposals, pred_objectness_logits, image_sizes, nms_thresh, pre_nms_topk, post_nms_topk, min_box_size, training)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mscores_per_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores_per_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mlvl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlvl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# filter empty boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/detectron2/structures/boxes.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(self, box_size)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mbox_size\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mclipping\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Box tensor contains infinite or NaN!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/media/indika/Sync/anaconda3/envs/detectron2/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 556317) is killed by signal: Killed. "
      ]
     }
    ],
diff --git a/V2/wandb/latest-run b/V2/wandb/latest-run
index 4bd0b72..63d5c59 120000
--- a/V2/wandb/latest-run
+++ b/V2/wandb/latest-run
@@ -1 +1 @@
-run-20211028_201107-2jukfznr
\ No newline at end of file
+run-20211028_203042-1dqs4kie
\ No newline at end of file
